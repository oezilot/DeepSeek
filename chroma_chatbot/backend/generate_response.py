'''
Dieses skript generiert zu einem promt eine antwort. 
es kann nur auf dige antworten die etwas mit den informationen in der chromadb haben antworten

folgendes macht das model:
- model initialisieren
- verbindung zur datenbank
- match von promt und datenbankeintrag finden
- deepseek generiert eine antwor mit den metadaten und dem promt
'''

import sys
sys.path.append("/home/zoe/Projects/DeepSeek/chroma")
from deepseek_embedding import string_to_tensor

import chromadb
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ðŸ“Œ pretrained Modell & Tokenizer laden
MODEL_PATH = "/home/zoe/Projects/DeepSeek/deepseek-llm-7b-chat" # hier befindet sich das model (weights, biases, dimensionen, alphabet, andere konfigs)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) # den tokenizer konfigurieren basierend des alphabets des modells (je nach model werden strings anders tokenisiert)
model = AutoModelForCausalLM.from_pretrained( # das model wird geladen (es wird ein Pytorch-modell erstellt mit den informationen aus dem config.js und den weights/biases)
    MODEL_PATH, torch_dtype=torch.bfloat16, device_map="auto"
)

# ðŸ“Œ Verbindung zur ChromaDB-Datenbank herstellen die bereits gefÃ¼llt ist mit den embeddings
client = chromadb.PersistentClient(path="/home/zoe/Projects/DeepSeek/knowledgebases")
collection = client.get_or_create_collection("Geschichten")

def deepseek_answer(query_text, n_results=1):

    query_vector = string_to_tensor(query_text).mean(dim=1).squeeze().tolist() # mit deepseek aus einem string einen semantischen vektor generieren

    query_results = collection.query( # chromadb nach dem semantisch Ã¤hnlichsten vektor durchsuchen
        query_embeddings=[query_vector],  
        n_results=n_results  # Anzahl der relevantesten Ergebnisse (in diesem fall nur eine geschihte und zwar die Ã¤hnlichte zurÃ¼ck geben)
    )

    # ðŸ”¹ Falls keine Ergebnisse gefunden wurden
    if not query_results["documents"]:
        return "Ich konnte leider keine relevante Information finden."

    # ðŸ”¹ Relevante Informationen aus ChromaDB abrufen (aus der sqlite datenbank)
    metadata = query_results["metadatas"][0][0]  # Die zugehÃ¶rigen Metadaten
    title = metadata.get("title", "Unbekannte Geschichte")  # Titel abrufen
    text = metadata.get("content", "Unbekannte Geschichte") # den text abrufen

    # ðŸ”¹ Nachrichten-Format fÃ¼r den DeepSeek-Prompt
    messages = [
        {"role": "system", "content": "Du bist ein hilfreicher KI-Assistent, der Fragen anhand von Hintergrundwissen beantwortet."},
        {"role": "user", "content": f"Hier ist eine relevante Geschichte:\n\nTitel: {title}\n\nInhalt: {text}\n\nNutze diese Information, um die folgende Frage mÃ¶glichst schÃ¶n und ausfÃ¼hrlich zu beantworten."},
        {"role": "user", "content": f"Frage: {query_text}"},
    ]

    input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to(model.device) # den promt fÃ¼r deepseek in tokenform bringen
    
    # ðŸ”¹ DeepSeek generiert eine Antwort
    with torch.no_grad():
        output = model.generate(input_ids, max_new_tokens=200)  # Antwort generieren

    # ðŸ”¹ Antwort dekodieren und ausgeben
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return answer

# ðŸ“Œ Beispielaufruf der Funktion
frage = "Wie heiÃŸt der Rock Frosch?"
antwort = deepseek_answer(frage)
print("ðŸ¤– DeepSeek Antwort:", antwort)